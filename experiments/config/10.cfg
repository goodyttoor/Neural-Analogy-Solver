{"optimizer": "Adam", "loss": "Binary_Crossentropy", "activation": "Sigmoid", "epochs": 1000, "batch": 64, "layers": [{"activation": "ReLU", "dropout": 0.1}, {"activation": "ReLU", "dropout": 0.1}, {"activation": "ReLU", "dropout": 0.1}, {"activation": "ReLU", "dropout": 0.1}, {"activation": "ReLU", "dropout": 0.1}, {"activation": "ReLU", "dropout": 0.1}, {"activation": "ReLU", "dropout": 0.1}, {"activation": "ReLU", "dropout": 0.1}, {"activation": "ReLU", "dropout": 0.1}, {"activation": "ReLU", "dropout": 0.1}]}